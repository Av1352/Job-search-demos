# ğŸ”¬ ClearML Experiment Tracking Demo

> Production-ready MLOps demonstration showcasing ClearML's auto-magical experiment tracking

**Project:** MNIST CNN Classification with Complete Experiment Management

---

## ğŸ¯ What This Demonstrates

This demo showcases ClearML's core MLOps capabilities through a complete training pipeline:

### ClearML Features Highlighted
- **âœ¨ Auto-Magical Tracking** - Just 2 lines of code to track everything
- **ğŸ“Š Real-time Metrics** - Loss, accuracy, and custom metrics
- **ğŸ”§ Hyperparameter Logging** - Automatic capture of all parameters
- **ğŸ¨ Model Versioning** - Automatic model storage and retrieval
- **ğŸ”„ Experiment Comparison** - Compare multiple runs side-by-side
- **ğŸ“ Git Integration** - Track code versions and uncommitted changes
- **ğŸš€ Reproducibility** - Clone any experiment with one click

---

## ğŸš€ Quick Start

### 1. Setup ClearML Account
```bash
# Sign up for free at https://app.clear.ml
# Get your API credentials from Settings > Workspace

# Initialize ClearML
clearml-init
```

### 2. Install Dependencies
```bash
cd Job-search-demos/clearml
pip install -r requirements.txt
```

### 3. Run Single Experiment
```bash
# Train with default hyperparameters
python train_mnist.py

# Or customize hyperparameters
python train_mnist.py --batch_size 128 --learning_rate 0.01 --epochs 10
```

### 4. Run Multiple Experiments
```bash
# Run 4 different configurations automatically
python train_experiments.py
```

### 5. View Results

Go to [https://app.clear.ml](https://app.clear.ml) and see:
- Real-time training metrics
- Experiment comparison dashboard
- Model artifacts and code
- GPU/CPU utilization
- Hyperparameter analysis

---

## ğŸ“Š Architecture
```
train_mnist.py
    â†“
Task.init()  â† Just 2 lines to enable tracking!
    â†“
[Automatic Logging]
    â”œâ”€â”€ Hyperparameters (from argparse)
    â”œâ”€â”€ Git info (commit hash, branch, diffs)
    â”œâ”€â”€ Environment (Python packages, versions)
    â”œâ”€â”€ Console output (all prints)
    â”œâ”€â”€ Scalars (loss, accuracy per epoch)
    â”œâ”€â”€ Models (PyTorch state_dicts)
    â””â”€â”€ Artifacts (any files you save)
    â†“
ClearML Server
    â†“
Beautiful Web Dashboard!
```

---

## ğŸ”¬ What Gets Tracked Automatically

### Without Any Extra Code:
- âœ… All hyperparameters from argparse
- âœ… Git commit hash and branch
- âœ… Uncommitted code changes
- âœ… Python environment (pip freeze)
- âœ… Console output (stdout/stderr)
- âœ… System metrics (CPU/GPU/RAM)

### With Simple Logger Calls:
- âœ… Training/validation metrics
- âœ… Custom scalars and plots
- âœ… Images and debug samples
- âœ… Confusion matrices
- âœ… Model weights

---

## ğŸ¨ Example Experiments

### Experiment 1: Baseline
```bash
python train_mnist.py \
  --batch_size 64 \
  --learning_rate 0.001 \
  --hidden_size 128 \
  --dropout 0.25 \
  --epochs 5
```

### Experiment 2: Higher Learning Rate
```bash
python train_mnist.py \
  --learning_rate 0.01
```

### Experiment 3: Larger Network
```bash
python train_mnist.py \
  --hidden_size 256
```

### Experiment 4: Less Regularization
```bash
python train_mnist.py \
  --dropout 0.1
```

**Result:** Compare all 4 in ClearML dashboard to find optimal config! ğŸ“ˆ

---

## ğŸ’¡ Key ClearML Advantages

### vs Manual Tracking (Spreadsheets)
- âŒ Manual: Copy-paste metrics to Excel
- âœ… ClearML: Automatic, real-time dashboard

### vs MLflow
- âŒ MLflow: Requires explicit logging calls everywhere
- âœ… ClearML: Auto-logs everything with Task.init()

### vs Weights & Biases
- âŒ W&B: SaaS-only, per-seat pricing
- âœ… ClearML: Open-source, self-hostable, free tier

---

## ğŸ† ClearML's Production Advantages

### For Teams:
- **Experiment Reproducibility** - Clone any experiment with one click
- **Collaboration** - Share experiments, compare results
- **Resource Management** - See GPU utilization across team
- **Model Registry** - Centralized model storage
- **Pipeline Automation** - Chain experiments together

### For Production:
- **Remote Execution** - Run experiments on cloud GPUs
- **Hyperparameter Optimization** - Automated tuning
- **Model Deployment** - Serve models from registry
- **A/B Testing** - Track model performance in production
- **Cost Tracking** - Monitor compute costs per experiment

---

## ğŸ“ˆ Results You'll See

After running experiments, you'll see in ClearML:

### 1. Experiment Table
| Experiment | LR | Hidden Size | Dropout | Test Acc | Train Time |
|-----------|----|----|-------|----------|------------|
| Baseline | 0.001 | 128 | 0.25 | 98.5% | 3m 45s |
| Higher LR | 0.01 | 128 | 0.25 | 98.2% | 3m 42s |
| Larger Hidden | 0.001 | 256 | 0.25 | 98.8% | 5m 12s |
| Less Dropout | 0.001 | 128 | 0.1 | 98.9% | 3m 48s |

### 2. Training Curves
- Loss over time (train vs test)
- Accuracy over time (train vs test)
- Side-by-side comparison of all experiments

### 3. System Metrics
- GPU utilization %
- Memory usage
- CPU usage

### 4. Model Artifacts
- Download best_model.pth
- See exact code used
- View full environment

---

## ğŸ› ï¸ Advanced Features

### Remote Execution
```bash
# Clone experiment #123 and run on remote GPU
clearml-task --clone 123 --queue gpu_queue
```

### Hyperparameter Optimization
```python
from clearml.automation import HyperParameterOptimizer

optimizer = HyperParameterOptimizer(
    base_task_id='your_task_id',
    hyper_parameters=[
        UniformParameterRange('learning_rate', min_value=0.0001, max_value=0.1),
        UniformIntegerParameterRange('hidden_size', min_value=64, max_value=512)
    ],
    objective_metric_title='test',
    objective_metric_series='accuracy',
    objective_metric_sign='max'
)

optimizer.start()
```

---

## ğŸ“š Technical Details

### Model Architecture
```
Conv2D(1â†’32, 3x3) â†’ ReLU â†’ MaxPool â†’ Dropout
Conv2D(32â†’64, 3x3) â†’ ReLU â†’ MaxPool â†’ Dropout
Flatten â†’ FC(3136â†’128) â†’ ReLU
FC(128â†’10) â†’ Output
```

**Parameters:** ~1.2M  
**Input:** 28x28 grayscale images  
**Output:** 10 classes (digits 0-9)

### Training Details
- **Dataset:** MNIST (60K train, 10K test)
- **Optimizer:** Adam
- **Loss:** CrossEntropyLoss
- **Batch Size:** 64 (configurable)
- **Epochs:** 5 (configurable)
- **Hardware:** CPU or CUDA GPU

---

## ğŸ¯ Why ClearML?

### The Problem ClearML Solves

**Before ClearML:**
```python
# Manually track everything ğŸ˜­
results = {
    'lr': 0.001,
    'batch_size': 64,
    'train_acc': 98.5,
    'test_acc': 97.2
}
with open('results.json', 'w') as f:
    json.dump(results, f)  # Hope you don't lose this file!
```

**With ClearML:**
```python
# Just add 2 lines ğŸ‰
task = Task.init(project_name='My Project', task_name='Experiment 1')
# Everything else is automatic!
```

---

## ğŸ‘¨â€ğŸ’» About This Demo

**Built by:** Anju Vilashni Nandhakumar  
**Purpose:** Application to ClearML  
**Contact:** nandhakumar.anju@gmail.com  
**LinkedIn:** [linkedin.com/in/anju-vilashni](https://www.linkedin.com/in/anju-vilashni/)  
**GitHub:** [github.com/Av1352](https://github.com/Av1352)

---

### Why ClearML?

I'm passionate about MLOps and building production-ready ML systems. ClearML's approach to "auto-magical" experiment tracking is exactly the kind of developer experience that accelerates ML teams. The platform's ability to track everything without code changes, combined with powerful orchestration and deployment features, makes it an ideal solution for scaling ML operations.

My background in medical imaging and deep learning has given me firsthand experience with the challenges of experiment tracking and model management. ClearML's comprehensive platform addresses these pain points elegantly.

---

**â­ Star this repo if you found this demo useful!**

*This is a technical demonstration project and is not affiliated with or endorsed by ClearML.*